创建项目：
    创建project：scrapy startproject [name]
    创建spider：1、进入项目目录
		2、scrapy genspider [name] [url]

项目格式：
    1、items.py：用来存放爬取下来的数据模型
    2、middlewares.py：用来存放各种中间件的文件
    3、pipelines.py：用来将items的模型存储到本地磁盘中
    4、settings.py：配置重的一些配置信息（请求头、请求频率、ip代理池等）
    5、scrapy.cfg：项目的配置文件
    6、spider包：存放所有爬虫代码

推荐设置项：
    1、ROBOTSTXT-OBEY = False
    2、DEFAULT_REQUEST_HEADERS 词条下添加 'User-Agent':
	"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 
	(KHTML, like Gecko) Chrome/72.0.3626.81 Safari/537.36 SE 2.X MetaSr 1.0"
    3、LOG_LEVEL='WARN'  
    4、ITEM_PIPELINES = {//激活pipeline
  	 'learnp2.pipelines.Learnp2Pipeline': 300,//优先级，值越小优先级越高
	}  

XPath：
    1、语法：
	1.1、/：直接子级  //：跳级
	1.2、@：属性 .：代替原xpath
	1.3、contains（）：包含  text（）：文字 last()：最后一个
	1.4、[]：条件框
    2、技巧
	2.1、string(.) 获取当下标签全部文本(防止<br>断文本) 二级父目录下//text（）获取所有
	2.2、在外侧.get()获取str  getall()获取所有list	 x="".join(x)获取字符串

运行爬虫
    scrapy crawl [spider name]

    from scrapy import cmdline
    cmdline.execute("scrapy crawl [spider name]".splite())
    
piplines.py应用：
    1、方法：
	def open_spider(self,spider):打开爬虫时调用
	def process_item(self,item,spider):传入数据时使用
	def close_spider(self,spider):爬虫结束时调用
    2、代码：
	from scrapy.exporters import JsonLinesItemExporter
	 class Learnp2Pipeline:
   	 def __init__(self):
   	     pass

   	 def open_spider(self,spoder):
   	     print('start crawl')
    	     self.fp = open("text.json", 'wb')#二进制打开
   	     self.exporter = JsonLinesItemExporter(self.fp,ensure_ascii=False,encoding='utf-8')

   	 def process_item(self,item,spider):
    	     self.exporter.export_item(item)
     	     return item

   	 def close_spider(self,spider):
      	     self.fp.close()
             print('close crawl')
	
items.py应用：
    1、xxx=scrapy.Field()
	